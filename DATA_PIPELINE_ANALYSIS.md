# Custom Prompt Training - ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¶„ì„

## ğŸ“Š ì „ì²´ ë°ì´í„° íë¦„ (Data Flow)

```
Dataset ìƒì„±
    â†“
VideoFeatureDataset (datasets/video_features.py)
    â”œâ”€ í´ë˜ìŠ¤ë³„ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
    â””â”€ .npy íŒŒì¼ ë¡œë“œ
        â†“
    use_video_level_pooling=False (ê¸°ë³¸ê°’)
    â”œâ”€ ê° ë¹„ë””ì˜¤ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë¶„í• 
    â”œâ”€ ê° ìœˆë„ìš°: [unit_duration, D]
    â””â”€ Strict normal filtering ì ìš©
        â†“
DataLoader
    â””â”€ Batch ìƒì„±
        â†“
train_one_epoch()
    â””â”€ features [B, T, D]
        â†“
VideoFeatureCLIP.forward()
    â””â”€ ì„ë² ë”© ë° ë¶„ë¥˜
        â†“
CrossEntropyLoss
    â””â”€ Backprop
```

---

## ğŸ” 1. ë°ì´í„°ì…‹ ë¡œë“œ ë‹¨ê³„ (custom_prompt_training.py)

### ì½”ë“œ ìœ„ì¹˜
**custom_prompt_training.py:161-182**

```python
train_dataset = VideoFeatureDataset(
    feature_dir=args.feature_dir,                    # í•™ìŠµ í”¼ì²˜ ë””ë ‰í† ë¦¬
    normal_class="Normal",                           # "Normal" í´ë˜ìŠ¤ëª…
    unit_duration=1,                                 # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í¬ê¸°: 1ì´ˆ (1í”„ë ˆì„)
    overlap_ratio=0.0,                               # ìœˆë„ìš° ì˜¤ë²„ë© ì—†ìŒ
    strict_normal_sampling=True,                     # ì—„ê²©í•œ ì •ìƒ í•„í„°ë§ ì ìš©
    use_video_level_pooling=False,                   # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‚¬ìš©
    verbose=True,
    seed=42,
)

classnames = train_dataset.classnames                # ['Normal', 'Abuse', 'Arrest', ...]
```

### ì¤‘ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê°’ | ì˜ë¯¸ |
|---------|-----|------|
| `unit_duration` | 1 | ê° ìœˆë„ìš°ê°€ 1í”„ë ˆì„ë§Œ í¬í•¨ |
| `overlap_ratio` | 0.0 | ìœˆë„ìš° ê°„ ì˜¤ë²„ë© ì—†ìŒ â†’ stride=1 |
| `strict_normal_sampling` | True | ë¹„ì •ìƒ ë¹„ë””ì˜¤ì—ì„œ ì´ë²¤íŠ¸ í›„ ì •ìƒ ë ˆì´ë¸” ìœˆë„ìš° ì œê±° |
| `use_video_level_pooling` | False | ë¹„ë””ì˜¤ ì „ì²´ í‰ê· í™” ì•ˆ í•¨ â†’ í”„ë ˆì„ë³„ ìƒ˜í”Œ ìƒì„± |

**ê²°ê³¼:** ê° í”„ë ˆì„ì´ ë…ë¦½ì ì¸ ìƒ˜í”Œ â†’ ë°ì´í„° ì–‘ ë§ìŒ â†’ ê³„ì‚°ëŸ‰ ë§ìŒ

---

## ğŸ“ 2. ìƒ˜í”Œ ìƒì„± ë‹¨ê³„ (VideoFeatureDataset._build_samples)

### ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ (use_video_level_pooling=False)

**ë°ì´í„°ì…‹ ìœ„ì¹˜:** `datasets/video_features.py:213-232`

```python
# ê° í´ë˜ìŠ¤ë³„ë¡œ ì²˜ë¦¬
for class_dir, label in self.class_to_label.items():  # e.g., "Abuse" â†’ label=1
    class_path = os.path.join(self.feature_dir, class_dir)

    # í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .npy íŒŒì¼
    npy_files = sorted([f for f in os.listdir(class_path) if f.endswith(".npy")])
    # ì˜ˆ: ["Abuse_1_x264.npy", "Abuse_2_x264.npy", ...]

    for npy_file in npy_files:
        npy_path = os.path.join(class_path, npy_file)
        self._process_npy_feature(
            npy_path, label, class_dir, is_normal_class, stride
        )
```

### ìœˆë„ìš° ë¶„í•  ë¡œì§

**ë°ì´í„°ì…‹ ìœ„ì¹˜:** `datasets/video_features.py:263-294`

```python
def _process_npy_feature(npy_path, label, class_dir, is_normal_class, stride):
    feat = np.load(npy_path, mmap_mode="r")  # [T, D], T = ë¹„ë””ì˜¤ ê¸¸ì´(ì´ˆ)
    total_seconds = feat.shape[0]            # T

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
    stride = max(1, int(unit_duration * (1.0 - overlap_ratio)))  # = 1
    num_windows = (total_seconds - unit_duration) // stride + 1
    # ì˜ˆ: T=1000, unit_duration=1, stride=1 â†’ 1000ê°œ ìœˆë„ìš°

    for i in range(num_windows):
        start_sec = i * stride           # 0, 1, 2, ..., 999
        end_sec = start_sec + 1          # 1, 2, 3, ..., 1000

        # Strict normal filtering ì ìš©
        if has_annotations and not is_normal_class and events:
            if not overlaps_event:
                if strict_normal_sampling and end_sec > earliest_event_start:
                    continue  # â† ì´ë²¤íŠ¸ í›„ ì •ìƒ ìœˆë„ìš° ì œê±°

        # ìƒ˜í”Œ ì¶”ê°€
        self.samples.append({
            "npy_path": npy_path,      # Abuse_1_x264.npy ê²½ë¡œ
            "start_sec": start_sec,     # 0, 1, 2, ...
            "end_sec": end_sec,         # 1, 2, 3, ...
            "label": label,             # 1 (Abuse)
            "video_id": "Abuse_1_x264", # íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
        })
```

**ê²°ê³¼:** ê° ë¹„ë””ì˜¤ì—ì„œ Tê°œ ìƒ˜í”Œ ìƒì„±

**ì˜ˆì‹œ:**
```
Abuse_1_x264.npy [1000, 512]
  â†’ [0:1], [1:2], [2:3], ..., [999:1000] (1000ê°œ ìƒ˜í”Œ)

Normal_1_x264.npy [2000, 512]
  â†’ [0:1], [1:2], ..., [1999:2000] (2000ê°œ ìƒ˜í”Œ)

...

ì´ ìƒ˜í”Œ ìˆ˜ = ëª¨ë“  ë¹„ë””ì˜¤ì˜ í”„ë ˆì„ í•©
```

---

## ğŸ”„ 3. ë°°ì¹˜ ìƒì„± ë‹¨ê³„ (DataLoader)

**custom_prompt_training.py:187-193**

```python
train_loader = DataLoader(
    train_dataset,
    batch_size=32,           # í•œ ë²ˆì— 32ê°œ ìƒ˜í”Œ
    shuffle=True,            # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì„ìŒ
    num_workers=4,           # 4ê°œ ë³‘ë ¬ ì²˜ë¦¬
    pin_memory=True,         # GPU ì „ì†¡ ìµœì í™”
)
```

**DataLoaderì˜ ë™ì‘:**
1. 32ê°œì˜ ì¸ë±ìŠ¤ ì„ íƒ (ë¬´ì‘ìœ„)
2. ê° ì¸ë±ìŠ¤ì— ëŒ€í•´ `VideoFeatureDataset.__getitem__(idx)` í˜¸ì¶œ
3. 32ê°œ ìƒ˜í”Œì„ Tensorë¡œ ë³€í™˜ ë° ë°°ì¹˜í™”

---

## ğŸ“¦ 4. ë°ì´í„° ë°˜í™˜ (VideoFeatureDataset.__getitem__)

**ë°ì´í„°ì…‹ ìœ„ì¹˜:** `datasets/video_features.py:325-339`

```python
def __getitem__(self, idx: int):
    sample = self.samples[idx]  # ìƒ˜í”Œ ë©”íƒ€ì •ë³´

    # .npy íŒŒì¼ ë¡œë“œ (ë©”ëª¨ë¦¬ ë§µ ì‚¬ìš©)
    feat = np.load(sample["npy_path"], mmap_mode="r")  # [T, D]

    if sample.get("pool_video", False):
        # ë¹„ë””ì˜¤ ë ˆë²¨ í‰ê· í™” (use_video_level_pooling=Trueì¼ ë•Œ)
        feature_vector = np.mean(feat, axis=0)         # [D]
        feature_tensor = torch.from_numpy(feature_vector).float()
    else:
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (use_video_level_pooling=Falseì¼ ë•Œ) â† í˜„ì¬
        window = feat[sample["start_sec"]:sample["end_sec"]]  # [1, 512]
        feature_tensor = torch.from_numpy(np.array(window)).float()

    # ë°˜í™˜: (features [unit_duration, D], label, video_id)
    return feature_tensor, sample["label"], sample["video_id"]
    # ì˜ˆ: (torch[1, 512], 1, "Abuse_1_x264")
```

**ë°˜í™˜ê°’:**
- `feature_tensor`: Shape [1, 512] (unit_duration=1, D=512)
- `label`: int (0-13, 14ê°œ í´ë˜ìŠ¤)
- `video_id`: str ("Abuse_1_x264" ë“±)

---

## ğŸ§  5. í•™ìŠµ ë‹¨ê³„ (train_one_epoch)

**ìœ„ì¹˜:** `train_video_feature_coop.py:171-211`

```python
def train_one_epoch(model, data_loader, optimizer, device, ...):
    for features, labels, _ in data_loader:  # â† 3ê°œ ê°’ ì–¸íŒ© (ìˆ˜ì •ë¨)
        features = features.to(device)       # [B, 1, 512] â†’ GPU
        labels = labels.to(device)           # [B] â†’ GPU

        # Forward pass
        logits = model(features)             # [B, 14]
        loss = F.cross_entropy(logits, labels)

        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        _, predicted = logits.max(1)         # [B]
        correct += predicted.eq(labels).sum().item()
```

---

## ğŸ¯ 6. ëª¨ë¸ ì²˜ë¦¬ (VideoFeatureCLIP.forward)

**ìœ„ì¹˜:** `trainers/video_feature_coop.py:446-475`

```python
def forward(self, features):  # features [B, 1, 512]
    # Temporal aggregation
    if features.dim() == 3:
        if self.temporal_agg == "mean":
            image_features = features.mean(dim=1)  # [B, 512]
        # ...

    # Generate text features
    prompts = self.prompt_learner()        # [14, seq_len, 512]
    tokenized_prompts = self.prompt_learner.tokenized_prompts
    text_features = self.text_encoder(prompts, tokenized_prompts)  # [14, 512]

    # L2 normalize
    image_features = image_features / image_features.norm(...)  # [B, 512]
    text_features = text_features / text_features.norm(...)    # [14, 512]

    # Cosine similarity
    logit_scale = self.logit_scale.exp()
    logits = logit_scale * image_features @ text_features.t()  # [B, 14]

    return logits
```

**ì²˜ë¦¬ ê³¼ì •:**
1. `[B, 1, 512]` â†’ Temporal mean â†’ `[B, 512]`
2. ê° í´ë˜ìŠ¤ë³„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± â†’ `[14, 512]`
3. Cosine similarity ê³„ì‚° â†’ `[B, 14]` logits

---

## ğŸ“Š ë°ì´í„° í¬ê¸° ì˜ˆì‹œ

### ì…ë ¥ êµ¬ì¡°
```
í•™ìŠµ ì…‹ êµ¬ì„±:
â”œâ”€ Normal/
â”‚  â”œâ”€ Normal_1_x264.npy [2000ì´ˆ, 512ì°¨ì›]
â”‚  â”œâ”€ Normal_2_x264.npy [1500ì´ˆ, 512ì°¨ì›]
â”‚  â””â”€ ... (20ê°œ ë¹„ë””ì˜¤)
â”œâ”€ Abuse/
â”‚  â”œâ”€ Abuse_1_x264.npy [1000ì´ˆ, 512ì°¨ì›]
â”‚  â””â”€ ... (20ê°œ ë¹„ë””ì˜¤)
â””â”€ ... (12ê°œ í´ë˜ìŠ¤)

ì´ ë¹„ë””ì˜¤: ~280ê°œ (14í´ë˜ìŠ¤ Ã— 20)
```

### ìƒ˜í”Œ ìƒì„±
```
VideoFeatureDataset (slding window, unit_duration=1)
  Normal: 2000 + 1500 + ... = ~40,000 ìƒ˜í”Œ
  Abuse:  1000 + 900 + ... = ~18,000 ìƒ˜í”Œ
  Arrest: ~25,000 ìƒ˜í”Œ
  ...

ì´ ìƒ˜í”Œ ìˆ˜: ~400,000+ (ëª¨ë“  ë¹„ë””ì˜¤ì˜ ì´ˆ í•©ì‚°)
```

### ë°°ì¹˜ ì²˜ë¦¬
```
DataLoader batch_size=32
  Iteration 1: 32ê°œ ìƒ˜í”Œ
  Iteration 2: 32ê°œ ìƒ˜í”Œ
  ...
  Epoch = 400,000 / 32 â‰ˆ 12,500 iterations
```

---

## ğŸš¨ í˜„ì¬ ë¬¸ì œ: Lossê°€ ë³€í•˜ì§€ ì•ŠëŠ” ì´ìœ 

### ê°€ëŠ¥ì„± 1: í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™” ë¬¸ì œ

**custom_prompt_training.py:256-258**
```python
ctx_init_str = " ".join(initial_prompts.get(cls, f"{cls}") for cls in classnames)
# ì˜ˆ: "a video showing physical abuse a video of police making arrest ..."

model = VideoFeatureCLIP(
    ...
    ctx_init=ctx_init_str,  # â† ë¬¸ì œ: ëª¨ë“  í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸ ì—°ê²°
    ...
)
```

**ë¬¸ì œ:**
- `ctx_init_str`ì´ ë„ˆë¬´ ê¸¸ì–´ì§ (ëª¨ë“  í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸ í•©ì¹¨)
- PromptLearnerê°€ ì´ë¥¼ **ê³µìœ  ì»¨í…ìŠ¤íŠ¸**ë¡œ ì‚¬ìš©
- ê²°ê³¼: ëª¨ë“  í´ë˜ìŠ¤ê°€ ë¹„ìŠ·í•œ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¡œ ì‹œì‘
- ë”°ë¼ì„œ text_featuresë„ ë¹„ìŠ·í•¨
- Loss ê°œì„  ë¶ˆê°€ëŠ¥

### ê°€ëŠ¥ì„± 2: ë°ì´í„° ë¶„í¬ ë¬¸ì œ

**ì •ìƒ í•„í„°ë§ì´ ê³¼ë„í•˜ê²Œ ì ìš©ë  ìˆ˜ ìˆìŒ:**
```
strict_normal_sampling=True
  + ë¹„ì •ìƒ ë¹„ë””ì˜¤ì—ì„œ ì´ë²¤íŠ¸ í›„ ì •ìƒ ìƒ˜í”Œ ì œê±°
  = ë¹„ì •ìƒ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ ê¸‰ê°
  = í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬í™”
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

```
[ ] 1. ë°ì´í„°ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ë‚˜?
    python -c "from datasets.video_features import VideoFeatureDataset; \
    ds = VideoFeatureDataset('...'); print(f'Samples: {len(ds)}')"

[ ] 2. ë°°ì¹˜ í˜•íƒœê°€ ì˜¬ë°”ë¥¸ê°€?
    python -c "from torch.utils.data import DataLoader; \
    loader = DataLoader(ds, batch_size=32); \
    batch = next(iter(loader)); print(batch[0].shape, batch[1].shape)"

[ ] 3. í”„ë¡¬í”„íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ë‚˜?
    â†’ debug_custom_prompt_init.py ì‹¤í–‰

[ ] 4. ê¸°ìš¸ê¸°ê°€ íë¥´ê³  ìˆë‚˜?
    â†’ debug_gradient_flow.py ì‹¤í–‰

[ ] 5. í…ìŠ¤íŠ¸ í”¼ì²˜ê°€ ë‹¤ì–‘í•œê°€?
    â†’ 14ê°œ í´ë˜ìŠ¤ì˜ text_featuresê°€ ì„œë¡œ ë‹¤ë¥¸ì§€ í™•ì¸
```

---

## ğŸ”§ ë‹¤ìŒ ë‹¨ê³„

1. **í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™” ë¬¸ì œ í™•ì¸**
   - `ctx_init_str` ê¸¸ì´ í™•ì¸
   - Text features ì°¨ì´ í™•ì¸

2. **ë°ì´í„° ë¶„í¬ í™•ì¸**
   - í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ì¶œë ¥
   - Strict filtering íš¨ê³¼ ì¸¡ì •

3. **Gradient flow í™•ì¸**
   - Loss ê°’ ë³€í™”
   - Prompt learner ì—…ë°ì´íŠ¸ í™•ì¸
